# Sudata-BCRA

AutomatizaciÃ³n de extracciÃ³n, integraciÃ³n y rÃ©plica de datos financieros desde la API oficial del Banco Central de la RepÃºblica Argentina (BCRA), junto con scraping de propiedades desde Zonaprop y una base de ventas interna.

## ğŸ“Œ Objetivo

El proyecto busca mantener sincronizacion entre dos bases de datos:
- Una base local (`localhost`).
- Una base en la nube (Supabase o Neon).
- Nota: 

AdemÃ¡s, enriquece la base con:
- **Cotizaciones histÃ³ricas del dÃ³lar oficial**, obtenidas desde la API del BCRA y almacenadas con actualizaciones incrementales.
- **Datos de inmuebles en venta en Posadas (Misiones)**, extraÃ­dos automÃ¡ticamente desde Zonaprop.

---

## âš™ï¸ TecnologÃ­as utilizadas

- **Lenguaje:** Python
- **LibrerÃ­as clave:** `requests`, `pandas`, `sqlalchemy`, `psycopg2`, `dotenv`, `selenium`
- **AutomatizaciÃ³n:** GitHub Actions, Programador de tareas (Windows)
- **Base de datos:** PostgreSQL (local y en la nube)
- **Web scraping:** Selenium + Chrome WebDriver (automÃ¡tico con `webdriver_manager`)

---

## ğŸ“ Estructura del proyecto

Sudata-BCRA/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ csv/ # Archivos CSV originales para la base de ventas
â”‚ â””â”€â”€ aux_publicacion_inmuebles.csv # Links de propiedades scrapeadas desde Zonaprop
â”‚
â”œâ”€â”€ logs/ # Logs de ejecuciÃ³n
â”‚
â”œâ”€â”€ scripts/ # Scripts modulares
â”‚ â”œâ”€â”€ bcra_fetch.py # Ingesta desde API del BCRA
â”‚ â”œâ”€â”€ csv_loader.py # Carga CSV â†’ Base de datos
â”‚ â”œâ”€â”€ add_foreing_keys.py # Agrega claves forÃ¡neas
â”‚ â”œâ”€â”€ extract.py # ExtracciÃ³n desde base local
â”‚ â”œâ”€â”€ load.py # Carga a base espejo
â”‚ â”œâ”€â”€ database.py # Motor de conexiÃ³n SQLAlchemy
â”‚ â”œâ”€â”€ logger.py # Utilidad de logging
â”‚ â”œâ”€â”€ web_scrapper.py # Extrae links de Zonaprop (scraping inicial)
â”‚ â””â”€â”€ data_scrapper.py # Extrae informaciÃ³n detallada de cada propiedad
â”‚
â”œâ”€â”€ main.py # Ejecuta BCRA + rÃ©plica
â”œâ”€â”€ main_bcra.py # Solo ingesta BCRA
â”œâ”€â”€ main_mirror.py # Solo rÃ©plica de base
â”œâ”€â”€ reset_and_reload.py # Reinicializa desde CSV
â”œâ”€â”€ DB_Mirror.py # RÃ©plica local automÃ¡tica (programador)
â”œâ”€â”€ requirements.txt # LibrerÃ­as requeridas
â”œâ”€â”€ .env # Variables de entorno (local, NO subir)
â””â”€â”€ .github/workflows/ # Acciones automatizadas

ğŸ“ scripts/
1. database.py
FunciÃ³n: Obtiene motores de conexiÃ³n SQLAlchemy para bases de datos.

Funciones principales:

get_engine(env_var_name: str): Devuelve un engine a partir de una variable de entorno (como ORIGIN_DB o DEST_DB).

Dependencias: os, sqlalchemy, dotenv.

2. extract.py
FunciÃ³n: Extrae todas las tablas de la base de datos origen (exceptuando algunas).

Funciones principales:

extract_all_tables(engine): Usa sqlalchemy.inspect para obtener dinÃ¡micamente los nombres de las tablas y extraer sus datos a un diccionario de DataFrames.

CaracterÃ­sticas especiales: Incluye una lista de exclusiÃ³n (excluded_tables) para omitir tablas como logs o metadatos.

Dependencias: pandas, sqlalchemy.

3. load.py
FunciÃ³n: Carga los DataFrames extraÃ­dos en la base de datos destino.

Funciones principales:

load_all_tables(data: dict, engine): Itera sobre un diccionario {nombre_tabla: DataFrame} y los sube a la DB con to_sql.

Dependencias: pandas.

4. bcra_fetch.py
FunciÃ³n: Realiza ingestas incrementales de cotizaciones oficiales del dÃ³lar desde la API del BCRA.

Funciones principales:

data_intake(): Crea/verifica la tabla cotizaciones, consulta cotizaciones nuevas por dÃ­a desde la Ãºltima fecha registrada, e inserta nuevos registros.

create_table(), get_last_date(), api_check(), data_process(), upload_data().

Log interno: Guarda en logs/bcra_fetch.log.

Seguridad: Usa verify=False por problemas de certificados SSL con la API del BCRA.

Dependencias: requests, pandas, sqlalchemy, dotenv.

5. csv_loader.py
FunciÃ³n: Crea automÃ¡ticamente tablas a partir de archivos CSV y las carga si no existen datos.

Funciones principales:

cargar_csvs_en_lote(directorio_csv): Lee todos los .csv del directorio y genera tablas basadas en tipos de datos, claves primarias y campos NOT NULL, definidos en config_columnas.

ConfiguraciÃ³n personalizada: Llave primaria y campos no nulos por tabla (config_columnas).

Dependencias: pandas, sqlalchemy, dotenv.

6. add_foreing_keys.py
FunciÃ³n: Verifica y agrega claves forÃ¡neas a la tabla factsales.

Funciones principales:

check_and_add_foreign_keys(): Si las claves no existen y los datos son consistentes, las crea.

Robustez: Incluye verificaciÃ³n de integridad referencial previa.

Dependencias: sqlalchemy, dotenv.

7. reset_and_reload.py
FunciÃ³n: Borra tablas especÃ­ficas y vuelve a crear y poblarlas desde archivos CSV.

Proceso completo:

Elimina tablas (DROP TABLE).

Llama a csv_loader para recrear tablas.

Ejecuta add_foreing_keys.py para establecer relaciones.

Dependencias: sqlalchemy, dotenv, scripts externos.

8. main.py
FunciÃ³n: Ejecuta el flujo completo de ingesta + extracciÃ³n + rÃ©plica.

Etapas:

data_intake() â€” Actualiza tabla cotizaciones.

extract_all_tables() â€” Extrae datos de todas las tablas Ãºtiles.

load_all_tables() â€” Replica a la base destino.

Logs: Guarda registro detallado en logs/main.log.

9. main_bcra.py
FunciÃ³n: Ejecuta Ãºnicamente la ingesta de cotizaciones desde BCRA.

Logs: Guarda en logs/main_bcra.log.

Uso: Ideal para programaciÃ³n semanal (ej. sÃ¡bados).

10. main_mirror.py
FunciÃ³n: Solo realiza la copia espejo entre la base local y la nube.

Logs: Guarda en logs/main_mirror.log.

Uso: ProgramaciÃ³n diaria mediante GitHub Actions o Windows Task Scheduler.

11. DB_Mirror.py
FunciÃ³n: Script de replicaciÃ³n diseÃ±ado para automatizaciÃ³n local (ej. Programador de tareas de Windows).

Flujo: Ejecuta extract_all_tables() y load_all_tables() diariamente desde localhost hacia Supabase.

Nota: Ideal para mantener sincronizada una base remota sin intervenciÃ³n manual.

12. logger.py
FunciÃ³n: Define una funciÃ³n log(msg) reutilizable que guarda logs en archivos .log con timestamp.

Uso compartido: Importado por main.py, main_bcra.py, main_mirror.py.



13. web_scrapper.py
FunciÃ³n: Permite buscar propiedades en Zonaprop segÃºn el tipo de inmueble (ej. `terrenos`, `casas`, `departamentos`, o combinaciones como `casas-terrenos`) y guarda los links de los anuncios encontrados.

- Entrada del usuario: tipo de inmueble.
- Salida:
  - `data/aux_publicacion_inmuebles.csv`: contiene los enlaces filtrados de propiedades tipo â€œclasificadoâ€.
bash: python scripts/web_scrapper.py


ğŸ—‚ï¸ Archivos adicionales importantes
.env: Contiene credenciales seguras para conexiÃ³n a bases (ORIGIN_DB, DEST_DB).
requirements.txt: Lista de dependencias del proyecto.
logs/: Carpeta donde se almacenan los .log de ejecuciÃ³n.
data/csv/: Carpeta con los archivos fuente .csv.

## âš™ï¸ Requisitos previos
Antes de ejecutar el proyecto, asegurate de tener instalado:

- [Python 3.11 o superior](https://www.python.org/downloads/)
- [Git](https://git-scm.com/)
- Acceso a 2 bases de datos PostgreSQL (local y/o remota)
- Navegador Chrome y compatibilidad con chromedriver (gestionado automÃ¡ticamente)

---

## ğŸ“¥ InstalaciÃ³n
Luego, creÃ¡ un archivo .env en la raÃ­z con las siguientes variables:
AbrÃ­ una terminal (CMD o PowerShell) y ejecutÃ¡ los siguientes pasos:

#Ingresar en terminal:
git clone https://github.com/Omega404/Sudata-mirror-DB.git
cd Sudata-mirror-DB
python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt

#Luego, crear un archivo .env en la raÃ­z con las siguientes variables:
ORIGIN_DB=postgresql://usuario:contraseÃ±a@localhost:5432/tu_base_local
DEST_DB=postgresql://usuario:contraseÃ±a@host_remoto:5432/tu_base_remota

ğŸš€ EjecuciÃ³n principal
#Ejecutar scraping inicial:
python scripts/web_scrapper.py

#Extraer datos detallados:
python scripts/data_scrapper.py

#Cargar CSV como tabla:
python -c "from scripts.csv_loader import cargar_csvs_en_lote; cargar_csvs_en_lote('data')"

#Ejecutar carga incremental + rÃ©plica:
python main.py

ğŸš€ Scripts principales
main.py - python main.py
Ejecuta el flujo completo:
Ingesta incremental desde la API del BCRA.
ExtracciÃ³n de todas las tablas desde la base local.
ReplicaciÃ³n a la base en la nube.
bash: python main.py

main_mirror.py
Replica todos los datos desde la base local hacia la base en la nube (sin ejecutar la API del BCRA).
bash: python main_mirror.py

main_bcra.py
Solo realiza la ingesta incremental de cotizaciones del dÃ³lar desde la API del BCRA y actualiza la tabla cotizaciones.
bash: python main_bcra.py

ğŸ”§ Scripts auxiliares

table_uploader.py 
Permite cargar archivos CSV individuales como tablas SQL.
âš ï¸ Precauciones:
Solo crea la tabla si no existe.
No reemplaza ni actualiza datos existentes.
python table_uploader.py nombre_del_archivo.csv

bcra_fetch.py
Script modular que consulta la API del BCRA y actualiza la tabla cotizaciones.
âš ï¸ Precauciones:
No debe ejecutarse en simultÃ¡neo con main.py o main_bcra.py.
bash: python bcra_fetch.py

csv_loader.py
Carga todos los archivos CSV ubicados en /data/csv/ y crea tablas automÃ¡ticamente.
âš ï¸ Precauciones:
Crea las tablas segÃºn la configuraciÃ³n de columnas y claves primarias.
No sobreescribe tablas existentes.
Ignora columnas no declaradas en la configuraciÃ³n.
bash: python -c "from scripts.csv_loader import cargar_csvs_en_lote; cargar_csvs_en_lote('data/csv')"

reset_and_reload.py
Elimina las tablas existentes en un orden seguro, las vuelve a crear desde los CSV y aplica las claves forÃ¡neas.
âš ï¸ Precauciones:
Borra completamente las tablas indicadas.
No solicita confirmaciÃ³n.
Solo debe usarse en entornos controlados (como desarrollo o restauraciÃ³n).
bash: python reset_and_reload.py
